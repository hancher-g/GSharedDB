import fractions
import time
import numpy as np
from av import AudioFrame, VideoFrame
import cv2
from enum import IntEnum
import asyncio

class AssemblerID(IntEnum):
    UnKnown = 0
    Audio = 1
    Video = 2


class IAssembler:
    def __init__(self, track_id: int, track_name: str):
        self.track_id = track_id
        self.track_name = track_name
        self.timestamp = 0
        self.starttime = 0.0
        self.reserve_frame = None
        self.frames = []
        self.time_base = fractions.Fraction(1, 1)  # Default time base
        self.offsets_timestamp = 0

    '''
        fps and sleep_time are used to calculate the next timestamp.
    '''
    def next_timestamp(self, pre_frame_state: bool = True) -> tuple[int, float]:
        raise NotImplementedError("Subclasses should implement this method.")

    def from_bytes(self, data, pre_frame_state: bool = True, is_default: bool = True):
        raise NotImplementedError("Subclasses should implement this method.")


class AudioStreamAssembler(IAssembler):
    def __init__(self, track_name: str):
        super().__init__(AssemblerID.Audio, track_name)
        self.samples_per_packet = 320  # Default value for samples per packet
        self.sample_rate = 16000  # Default sample rate
        self.time_base = fractions.Fraction(1, self.sample_rate)
        self.reserve_frame = np.random.randint(-30, 30, self.samples_per_packet, dtype=np.int16)

    def next_timestamp(self, pre_frame_state: bool = True) -> tuple[int, float]:
        wait_time = 0.0
        if self.starttime == 0:
            self.starttime = time.time()
            self.timestamp = 0
        else:
            if pre_frame_state:
                self.timestamp += self.samples_per_packet 
            wait_time = self.starttime + (self.timestamp / self.sample_rate) - time.time()            
        return self.timestamp, wait_time

    def from_bytes(self, stream: bytes, pre_frame_state: bool = True, is_default: bool = True, format: str = "s16", layout: str = "mono"):
        audio_data = None

        if stream is None or len(stream) == 0:
            if is_default:
                audio_data = self.default_frame()
            else:
                print("No data provided to create AudioFrame.")
                return None
        else:
            audio_data = np.frombuffer(stream, dtype=np.int16)

        audio_data = audio_data.reshape((1, len(audio_data)))
        pts, wait_time = self.next_timestamp(pre_frame_state)
        audio_frame = AudioFrame.from_ndarray(audio_data, format=format, layout=layout)
        audio_frame.sample_rate = self.sample_rate
        audio_frame.pts = pts
        audio_frame.time_base = self.time_base
        return audio_frame, wait_time


class VideoStreamAssembler(IAssembler):
    def __init__(self, tracker_name: str, width: int, height: int):
        super().__init__(AssemblerID.Video, tracker_name)
        self.width = width
        self.height = height
        self.create_default_frame(width, height)
        self.clock_rate = 45000
        self.ptime = 1 / 25 # packetization Time 
        self.time_base = fractions.Fraction(1, self.clock_rate)

    def create_default_frame(self, width, height):
        if self.reserve_frame is None:
            self.reserve_frame = np.zeros((width, height, 3), dtype=np.uint8)
        if self.reserve_frame.shape[0] != height or self.reserve_frame.shape[1] != width:
            self.reserve_frame = cv2.resize(self.reserve_frame, (width, height), interpolation=cv2.INTER_LINEAR)
        img = self.reserve_frame.copy()

        def draw_text(img, text, font, font_scale, font_thickness, text_center_x, text_center_y):
            text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]
            text_x = int(text_center_x - text_size[0] / 2)
            text_y = int(text_center_y + text_size[1] / 2)
            cv2.putText(img, text, (text_x, text_y), font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)

        # Get the size of the text
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1 # 0.5 * width / 640

        # Get the current time
        current_time = time.strftime("%Y:%m:%d %H:%M:%S")
        draw_text(img, current_time, font, font_scale, 1, width / 2, height * 765 / 960)
        return img

    def next_timestamp(self, pre_frame_state: bool = True) -> tuple[int, float]:
        wait_time = 0.0        
        if self.starttime == 0:
            self.starttime = time.time()
            self.timestamp = 0
        else:
            
            if pre_frame_state:
                self.timestamp += int(self.ptime * self.clock_rate) + self.offsets_timestamp
                
            wait_time = self.starttime + (self.timestamp / self.clock_rate) - time.time()
        return self.timestamp, wait_time

    '''
        Convert bytes to VideoFrame.
        判断上一帧是否存在，如果不存在则创建一个默认的帧。
        使用上一帧的时间戳和时间基准来创建新的 VideoFrame。
    '''
    def from_bytes(self, stream: bytes, pre_frame_state: bool = True, is_default: bool = True, format: str = "rgb24"):
        video_data = None

        if stream is None or len(stream) == 0:
            if is_default:
                video_data = self.create_default_frame(self.width, self.height)
            else:
                print("No data provided to create VideoFrame.")
                return None
        else:
            video_data = np.frombuffer(stream, dtype=np.uint8)
            video_data = video_data.reshape((self.height, self.width, 3))

        pts, wait_time = self.next_timestamp(pre_frame_state)
        video_frame = VideoFrame.from_ndarray(video_data, format=format)
        video_frame.pts = pts
        video_frame.time_base = self.time_base
        return video_frame, wait_time


class MediaAssemblerManager:
    def __init__(self):
        self.tracks = {}

    def add(self, track: IAssembler):
        self.tracks[track.track_name] = track

    def get(self, track_name: str) -> IAssembler | None:
        return self.tracks.get(track_name)

    def remove(self, track_name: str):
        if track_name in self.tracks:
            del self.tracks[track_name]
            
    def close(self):           
        self.tracks.clear()
